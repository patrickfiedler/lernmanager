# Artifact Feedback & Step Checkboxes â€” Design Plan

**Status:** Draft â€” needs refinement
**Date:** 2026-02-24
**Origin:** Design discussion session

---

## Problem

Students are overwhelmed by wall-of-text task descriptions (10â€“25 lines). They also get no quality feedback on their growing artifact until the teacher manually grades the capstone â€” by which point errors have compounded across multiple tasks.

## Design Decisions Made

### 1. Per-step checkboxes (process tracking)

Numbered steps inside `ðŸ“‹ Aufgabe:` get checkboxes. Students tick off steps as they work.

- All steps remain visible (not progressive disclosure â€” students need to skim and refer back)
- State persisted in DB (school computers clear localStorage)
- Purely for student self-tracking â€” not logged to teacher, not grading-relevant
- Orthogonal to artifact grading

**Open:** new DB table needed for step-completion state (per student, per subtask, per step index)

### 2. Per-task artifact upload with formative LLM feedback

After completing the steps of any task that produces artifact output, students can upload their file. The LLM checks it against a task-scoped rubric and returns checklist-style feedback.

- Upload is **optional at every stage** â€” no gating, no blocking
- Feedback is **formative only** â€” no numeric score shown to students
- Format: âœ“/âœ— per observable criterion + short explanation
- Students can re-upload after fixing issues
- The rubric **grows organically** â€” task N rubric = task N-1 criteria + new criteria for task N

### 3. Teacher sets the final grade

The teacher manually reviews the capstone submission and enters the grade. The LLM is a tool, not an authority. The grade number comes only from the teacher.

| Layer | What it does | Binding? |
|-------|-------------|---------|
| Per-step checkboxes | Student tracks own process through a task | No |
| Per-task artifact upload | LLM checks criteria for tasks done so far | No â€” formative |
| Capstone upload | LLM checks all unit criteria | No â€” teacher reviews |
| Teacher grade entry | Sets the actual grade in the system | Yes |

---

## Two Grading Flows

These are distinct, complementary, and serve different purposes.

### Flow A â€” In-app, student-initiated (formative)

Student uploads during or after a task â†’ text extracted + pseudonymized â†’ preview shown â†’ (if LLM feedback enabled) LLM checks against growing rubric â†’ checklist feedback shown â†’ student can fix and re-upload.

- No batching. Triggered on upload.
- Student is already authenticated â€” filename is irrelevant for identity. It's just a criterion to check.
- Runs via extended `llm_grading.py`, standard Anthropic API (no batch discount).
- **LLM feedback is opt-in per class** (see Privacy section). Upload + preview always work regardless.

### Flow B â€” Teacher-initiated batch (summative)

Teacher runs PowerShell file collection â†’ grading-with-llm CLI on teacher's machine â†’ results pushed to Lernmanager API â†’ teacher reviews â†’ sets grade.

- grading-with-llm runs unchanged. No Lernmanager server involvement.
- Uses Batch API (50% cost discount, async).
- Natural trigger: end of unit, not per-task.

### Why not batch-only?

Self-paced students finish tasks at different times â€” there's never a natural class-wide collection moment. For 10-12 year olds, feedback arriving next lesson is pedagogically weak: emotional connection to the work is gone, motivation to revise drops sharply. Flow A closes the per-task loop immediately; Flow B handles the final summative grade at the teacher's discretion.

---

## File Management

Students in grades 5/6 are just learning file handling. The concern is that early uploads will be messy (wrong filename, wrong format, wrong file entirely).

**This is not a blocker â€” it's a learning mechanism.** The filename is already a `fertig_wenn` criterion ("Deine Datei heiÃŸt genau `Digitaler-Kompass-[Vorname].pptx`"). When the student uploads the wrong filename, they see it immediately in the preview â€” before any API call. The first few failed checks teach file discipline at low stakes.

For Flow B identity is resolved by filename (network login: `mueller.anna.pptx`) â€” this is the existing grading-with-llm convention and is unchanged.

**Files are not stored on the Lernmanager server.** The uploaded file is processed in memory: metadata stripped, text extracted, student name pseudonymized. Only the extracted text and feedback result are stored (in `artifact_feedback` table). The original file is discarded after extraction. For Flow B, the teacher collects files directly from network folders â€” Lernmanager is not a file store.

---

## Privacy & DSGVO

### Two distinct processing activities

| Activity | Data stays | Legal basis | Status |
|----------|-----------|-------------|--------|
| Upload + extract + preview | Lernmanager server only | Art. 6 lit. e (Bildungsauftrag) | Always available |
| Send to Anthropic API | Leaves school infrastructure | Needs clarification | **Opt-in per class** |

Keeping these separate means the feature can ship and be used without resolving the API question first.

### Pseudonymization before any API call

Adapted from grading-with-llm's anonymization layer:

1. **Strip document metadata** â€” `.pptx` author/creator fields removed via `python-pptx` before any storage
2. **Replace student name in text** â€” first name, last name, and combinations replaced with `[SchÃ¼ler/in]` before API call. Student identity is already known from the session; pseudonym is only for the API payload.
3. **No original file stored** â€” only extracted + cleaned text stored server-side

### Student preview (transparency)

Before any API call, the student sees the extracted + pseudonymized text in a collapsible section:

> *"Das sieht die KI: [extracted text with name replaced]. Dein Name und persÃ¶nliche Daten wurden ersetzt."*

This serves both DSGVO transparency and pedagogy â€” Unit 4 is literally about data privacy. Seeing this in action is a teaching moment.

### Opt-in: teacher enables LLM feedback per class

Admin setting per Klasse: "KI-Aufgabencheck aktiviert" (off by default).

- When off: upload + preview work, no API call made, no feedback shown
- When on: full Flow A with checklist feedback
- Teacher is responsible for having resolved DSGVO requirements before enabling

### Legal basis analysis

**Art. 6 lit. f (berechtigtes Interesse) â€” not available.** Public schools are public authorities; lit. f explicitly excludes them. Common misconception.

**Art. 6 lit. e (Ã¶ffentliche Aufgabe) â€” correct basis, with conditions.** School education is a public task. ThÃ¼rSchulG Â§70 authorises data processing for educational purposes. Parental consent (Einwilligung) is therefore NOT required as the primary legal basis for routine educational processing. What parents are owed is transparency (Art. 13 DSGVO notification), not consent.

However, "necessary" is a real constraint. German DSBs interpret it strictly. A local alternative that achieves the same purpose weakens the "necessary" argument for cloud API specifically â€” see Ollama note below.

**The harder problem: Chapter V (international transfers).** Anthropic is a US company. Even if Art. 6 lit. e covers the purpose, the transfer to a non-EU country needs a separate basis:
- **EU-US Data Privacy Framework (DPF):** valid if Anthropic is certified â€” needs verification
- **Standard Contractual Clauses (SCCs):** likely included in Anthropic's DPA as fallback

German DSBs have been consistently strict on US cloud services in schools. Several states have restricted Google Workspace and Microsoft 365 for exactly this reason. Thuringia's DSB may have specific school cloud guidance â€” check before enabling.

### What must be done before enabling the feature

| Step | What | Status |
|------|------|--------|
| DPA (Art. 28 AVV) | Contract with Anthropic as Auftragsverarbeiter | Not done â€” Anthropic provides one |
| Transfer basis | Verify Anthropic DPF certification or confirm SCCs in DPA | Needs verification |
| Confirm no training use | Anthropic API must not use inputs for model training â€” verify in DPA | Needs verification |
| DPIA (Art. 35) | Likely required for systematic LLM processing of minor's work | Needs DSB input |
| ThÃ¼rDSB guidance | Thuringia may have specific school cloud service rules | Check |
| School privacy notice | Add LLM processing to DatenschutzerklÃ¤rung (Art. 13 obligation) | Required |
| DSB / Schulleitung sign-off | School's own data protection officer should approve | Required |

**Nothing above is optional. The feature must not be enabled for any class until all steps are complete.**

### Why Ollama is not a practical alternative

grading-with-llm supports Ollama (local LLM, school-controlled infrastructure) which would avoid the Chapter V transfer problem entirely. However:
- Accuracy: 93.2% vs. 98.3% for Haiku 4.5 on the same task
- Speed: matching Haiku's throughput (~10 students in 1.2 min) requires significant GPU hardware (7B model minimum, 14B for better accuracy) â€” far beyond a school VPS budget
- Maintenance: running a local LLM server adds operational complexity

Ollama is a useful fallback for development and testing, not a production substitute at this scale. The DSGVO problem must be solved for the cloud API â€” not worked around with an underpowered local alternative.

Note: Unit 4 ("Digitaler Kompass") involves students' personal internet habits and privacy opinions â€” the most sensitive content in the curriculum. Extra care warranted specifically for this unit.

---

## Datenschutz & DSGVO â€” Rechtliche Analyse

> VollstÃ¤ndige Analyse auch in `docs/research/2026-02-07_learning_paths_and_quiz_evolution.md` Abschnitt 6.

### Warum das Artefakt-Feedback rechtlich anders ist als die Quiz-Bewertung

Die bestehende LLM-Quizbewertung fÃ¤llt **nicht unter die DSGVO**: Es werden nur anonyme Daten gesendet (Frage + kurze Antwort, kein Identifier). Das Artefakt-Feedback ist anders â€” vollstÃ¤ndige Dokumente kÃ¶nnen persÃ¶nliche Inhalte und den Namen des SchÃ¼lers enthalten. Die DSGVO gilt hier wahrscheinlich.

### Rechtsgrundlage

**Art. 6 lit. f (berechtigtes Interesse) â€” nicht anwendbar.** Ã–ffentliche Schulen sind BehÃ¶rden; lit. f gilt ausdrÃ¼cklich nicht fÃ¼r BehÃ¶rden bei der AufgabenerfÃ¼llung. HÃ¤ufiger Irrtum.

**Art. 6 lit. e (Ã¶ffentliche Aufgabe / Bildungsauftrag) â€” korrekte Grundlage.** Schulische Bildung ist eine Ã¶ffentliche Aufgabe. ThÃ¼rSchulG Â§70 erlaubt Datenverarbeitung fÃ¼r Bildungszwecke. **Elterneinwilligung ist nicht erforderlich** â€” und wÃ¤re als Grundlage schlechter (widerrufbar, MachtgefÃ¤lle in der Schule). Eltern haben stattdessen ein Recht auf Information (Art. 13), nicht auf Zustimmung.

### EU-Anbieter vs. US-Anbieter

Die DSGVO unterscheidet zwischen der *Rechtsgrundlage der Verarbeitung* (Art. 6) und dem *Ort der Verarbeitung* (Kapitel V). Beides muss stimmen.

**Bei US-Anbietern (Anthropic)** greift Kapitel V: Jede Ãœbermittlung personenbezogener Daten in ein Drittland braucht eine eigene Grundlage (DPF-Zertifizierung oder SCC). ZusÃ¤tzlich gilt der **CLOUD Act**: US-BehÃ¶rden kÃ¶nnen US-Unternehmen zur Herausgabe von Daten verpflichten â€” auch wenn diese auf EU-Servern liegen. Das gilt ebenso fÃ¼r AWS EU, Azure EU und Google Cloud EU.

**Bei EU-Anbietern (Mistral, OVHcloud)** entfÃ¤llt Kapitel V vollstÃ¤ndig â€” eine Ãœbermittlung von Deutschland nach Frankreich ist keine DrittlandÃ¼bermittlung. Der CLOUD Act gilt nicht.

| Pflicht | US-Anbieter (Anthropic) | EU-Anbieter (Mistral / OVHcloud) |
|--------|------------------------|----------------------------------|
| Art. 6 lit. e Rechtsgrundlage | âœ“ erforderlich | âœ“ erforderlich |
| AV-Vertrag (Art. 28) | âœ“ erforderlich | âœ“ erforderlich |
| Kapitel V (DrittlandÃ¼bermittlung) | âœ“ DPF / SCC prÃ¼fen | **entfÃ¤llt** |
| CLOUD Act Risiko | âœ“ vorhanden | **entfÃ¤llt** |
| DPIA (Art. 35) | wahrscheinlich erforderlich | wahrscheinlich nicht nÃ¶tig |
| Informationspflicht (Art. 13) | âœ“ erforderlich | âœ“ erforderlich |
| Verarbeitungsverzeichnis (Art. 30) | âœ“ erforderlich | âœ“ erforderlich |
| DSB / Schulleitung | âœ“ empfohlen | âœ“ empfohlen |

### Was vor der Aktivierung fÃ¼r eine Klasse erledigt sein muss

**Bei EU-Anbieter (empfohlen: OVHcloud oder Mistral):**
- [ ] AV-Vertrag (Art. 28) mit gewÃ¤hltem Anbieter abschlieÃŸen
- [ ] Keine Modelltrainierung mit API-Eingaben â€” im AV-Vertrag bestÃ¤tigen
- [ ] DatenschutzerklÃ¤rung der Schule ergÃ¤nzen (Art. 13)
- [ ] Verarbeitungsverzeichnis aktualisieren (Art. 30)
- [ ] DSB kurz informieren â€” DPIA wahrscheinlich nicht erforderlich, aber absichern
- [ ] Genehmigung Schulleitung

**ZusÃ¤tzlich bei US-Anbieter (Anthropic):**
- [ ] DPF-Zertifizierung prÃ¼fen oder SCC im AV-Vertrag bestÃ¤tigen
- [ ] ThÃ¼ringer DSB-Leitlinien zu Schul-Cloud-Diensten prÃ¼fen
- [ ] Datenschutz-FolgenabschÃ¤tzung (DPIA, Art. 35) durchfÃ¼hren

**Das Feature darf fÃ¼r keine Klasse aktiviert werden, bevor diese Schritte abgeschlossen sind.**

### Hinweis zu Einheit 4

â€žDigitaler Kompass" enthÃ¤lt SchÃ¼lerreflexionen zu eigenen Online-Erfahrungen und Datenschutzgewohnheiten â€” inhaltlich das Sensibelste im Curriculum. Gleichzeitig ist die Vorschau des pseudonymisierten Textes vor dem API-Aufruf ein direktes Unterrichtsbeispiel fÃ¼r das Thema Datenschutz.

---

## LLM Backend Options

Two backends are supported. The choice determines DSGVO complexity.

Cost estimates below assume 70 students Ã— 2 uploads/week, ~1000 input + ~150 output tokens per student (560 requests/month).

| Provider | EU company? | Model | Input/MTok | Output/MTok | Est. â‚¬/month |
|----------|------------|-------|-----------|------------|-------------|
| Anthropic | No (US) | Haiku 4.5 | $0.80 | $4.00 | ~â‚¬1.20 |
| Mistral AI | Yes (FR) | Large 3 | â‚¬0.50 | â‚¬1.50 | ~â‚¬0.40 |
| OVHcloud | Yes (FR) | Llama-3.3-70B | â‚¬0.67 | â‚¬0.67 | ~â‚¬0.43 |
| **OVHcloud** | **Yes (FR)** | **Qwen3-32B** | **â‚¬0.08** | **â‚¬0.23** | **~â‚¬0.06** |
| OVHcloud | Yes (FR) | Mistral-Nemo-12B | â‚¬0.13 | â‚¬0.13 | ~â‚¬0.08 |
| Mac Mini + WireGuard | Yes (local) | Qwen2.5-14B | â‚¬599 one-time | â€” | ~â‚¬10 amort. |

### Option A: Anthropic cloud API (Haiku 4.5)

- Accuracy: 98.3%, speed: ~10 students in 1.2 min
- Cost: ~â‚¬1.20/month
- **DSGVO blocker:** Chapter V international transfer to US company â€” requires DPA, DPF/SCC verification, DPIA, DSB sign-off before use (see Privacy section)

### Option B-1: Mistral AI (Paris, France)

- EU company â€” no CLOUD Act risk, no Chapter V transfer (EUâ†’EU)
- DPA (Data Processing Addendum) explicitly available
- ISO 27001 certified, strong German language support
- Mistral Large 3: â‚¬0.50/â‚¬1.50 per MTok â†’ ~â‚¬0.40/month â€” cheaper than Haiku at flagship quality
- DSGVO process: DPA + Verarbeitungsverzeichnis + Art. 13 notice + DSB sign-off. No DPIA likely needed. Significantly lighter than Anthropic path.

### Option B-2: OVHcloud AI Endpoints (Roubaix, France) â€” recommended

- EU company, 100% EU infrastructure, no US dependencies, ISO 27001
- Runs open-source models: Qwen3, Llama 3.3, Mistral variants
- **Qwen3-32B: â‚¬0.08/â‚¬0.23 per MTok â†’ ~â‚¬0.06/month** â€” 32B reasoning model at negligible cost
- Qwen3 reasoning mode (extended thinking) likely excellent for structured checklist grading
- DSGVO process: same as Mistral â€” DPA + transparency + DSB sign-off, no DPIA likely needed
- Open question: German language quality of Qwen3-32B on OVHcloud vs. Mistral Large â€” benchmark needed

### Option C: Ollama on teacher's Mac Mini via WireGuard

Ollama runs on teacher's home Mac Mini. Production server reaches it via WireGuard tunnel.

**DSGVO situation:** The Mac Mini is teacher's own device â€” legally equivalent to the teacher's laptop. Pseudonymized artifact text flowing over an encrypted WireGuard tunnel to the home machine and back is no different from downloading a CSV of grades. No third-party processor â†’ no Art. 28 AV-Vertrag for the LLM component. Chapter V international transfer problem disappears entirely. Disk encryption + strong password on Mac Mini required (same as existing LehrergerÃ¤t mitigations).

**Performance on Apple Silicon** â€” significantly better than grading-with-llm's 93.2% CPU benchmark:

| Hardware | Model | Speed | Notes |
|----------|-------|-------|-------|
| School VPS (CPU) | qwen2.5:7b | slow | What grading-with-llm benchmarked |
| M4 Mac Mini (16GB) | 14B 4-bit | ~15 tok/s | Fast enough for real-time use |
| M4 Pro Mac Mini (24GB+) | 32B 4-bit | ~10 tok/s | Near-cloud accuracy territory |

For grading (short prompts, short outputs), 15 tok/s is sufficient. Accuracy gap vs. Haiku narrows with larger models on Apple Silicon â€” benchmark with actual Unit 4 rubric before committing to a model.

**WireGuard + dynamic home IP:** Mac Mini initiates the tunnel outbound to the VPS (static IP). VPS never needs to reach the home IP directly. Standard setup, well-documented.

**Operational dependency:** Mac Mini must be on and tunnel active during class for Flow A (per-task formative). Flow B (batch capstone) is teacher-initiated â€” turn it on when needed. If Mac Mini is unreachable, artifact feedback shows "zurzeit nicht verfÃ¼gbar, spÃ¤ter nochmal versuchen" â€” no silent failure.

**Cost note:** Mac Mini M4 base ~â‚¬700. Anthropic API ~â‚¬1.20/month â†’ financial break-even is never. The value is avoiding months of DSGVO bureaucracy (DPA, DPIA, DSB consultation, school approval), not cost savings.

**Code changes for Option B:**
- `config.py`: add `LLM_ARTIFACT_ENDPOINT` (WireGuard IP of Mac Mini, e.g. `http://10.0.0.2:11434`)
- Artifact grading module: use Ollama client when endpoint configured, Anthropic when not
- Graceful offline message when Mac Mini unreachable
- Pattern already exists in `llm_grading.py` â€” Ollama support is not new

**Hardware recommendation: Mac Mini M4 base (16GB, â‚¬599)**

Apple Silicon unified memory â€” unlike traditional PCs, all 16GB is shared between CPU, GPU, and Neural Engine. The full pool is available for the model.

| Model | Memory (Q4) | Fits in 16GB? | Notes |
|-------|------------|--------------|-------|
| Qwen2.5:7B | ~4.5 GB | âœ“ comfortably | Good, possibly sufficient |
| Qwen2.5:14B | ~8.5 GB | âœ“ well | Recommended target |
| 32B | ~20 GB | âœ— | Needs 24GB M4 Pro |

macOS uses ~3â€“5 GB, leaving 11â€“13 GB for model + context. 14B fits cleanly. Context per grading call is small (rubric ~500 tokens + document ~500â€“1500 tokens + output ~150 tokens) â€” KV cache overhead is minimal.

Speed on M4 (Metal GPU acceleration): ~25â€“40 tok/s for 14B. A single student submission with all criteria in one call: ~5â€“10 seconds â€” acceptable for Flow A (student waits). For Flow A, one combined checklist call is preferred over 6 sequential micro-prompts; slight accuracy trade-off but appropriate for formative feedback.

M4 Pro (24GB, ~â‚¬1,399) would open 32B models but is not justified for this use case. 14B on 16GB gives sufficient quality, good speed, and clean DSGVO at less than half the price.

**Open:** benchmark accuracy of Qwen2.5:14B on Unit 4 rubric before committing.

---

## Code Reuse: grading-with-llm

**Reuse as a pattern, not a direct dependency.**

| Component | Reuse approach |
|-----------|---------------|
| `src/file_handlers/docx_processor.py` | Already works â€” adapt into Lernmanager directly |
| `src/file_handlers/odt_processor.py` | Already works â€” adapt into Lernmanager directly |
| `src/llm/prompt_builder.py` + `src/grading/micro_grader.py` | Copy micro-prompting pattern into extended `llm_grading.py` |
| `src/llm/anthropic_client.py` | Not needed â€” Lernmanager already calls Anthropic API directly |
| Batch API + CLI workflow | Flow B only â€” grading-with-llm runs as-is on teacher's machine |

## Supported File Formats

| Format | Library | What's extracted | Notes |
|--------|---------|-----------------|-------|
| `.docx` | `python-docx` | Paragraphs + tables | Already in grading-with-llm â€” direct reuse |
| `.odt` | `odfpy` | Paragraphs + tables | Already in grading-with-llm â€” direct reuse |
| `.pptx` | `python-pptx` | Slide text per slide | New dependency needed on Lernmanager server |
| `.odp` | `python-pptx` or unzip+XML | Slide text per slide | Same pattern as .pptx |
| `.sb3` | `zipfile` + `json` | Block graph â†’ readable summary | Feasible but needs transformation layer (see below) |

### .sb3 (Scratch 3 project files)

`.sb3` is a ZIP archive. Inside:
- `project.json` â€” full project: sprites, script block trees, variables, comments, costume names
- Asset files (costume images, sounds) â€” binary, ignored for text grading

The Scratch 3 spec is fully open. The challenge: `project.json` stores scripts as a flat dictionary of blocks with opaque IDs (`"next": "abc123"`). Raw JSON sent to an LLM is unreadable noise. A **transformation layer** is needed to convert the block graph into a human-readable summary:

```
Sprite "Katze":
  Skript 1: Wenn [Flagge] â†’ wiederhole 10 mal â†’ bewege 10 Schritte
  Skript 2: Wenn [Leertaste] â†’ sage "Hallo" 2 Sekunden
  KostÃ¼me: katze1, katze2
Variablen: Punkte
BÃ¼hne: 1 Hintergrund
```

For grades 5/6, rubric criteria are structural and checkable from such a summary:
- "Has at least 2 sprites" âœ“
- "Has a loop block (repeat/forever)" âœ“
- "Has a conditional (if/else)" âœ“
- "Uses at least one variable" âœ“
- "Has comments" âœ“

**Assessment:** feasible, but the block-graph-to-summary transformer is ~1â€“2 days of engineering work. Worth doing â€” Scratch projects appear in multiple units. Not needed for the first implementation (Unit 4 uses .pptx/.odp).

**Open:** should `.sb3` support be built as a standalone module shareable with grading-with-llm?

---

## Cost Estimate â€” Unit 4 "Digitaler Kompass"

**Baseline (measured):** grading-with-llm reports ~$0.007 per 10 students per run (Haiku 4.5, Batch API, text-only, 6 criteria, similar rubric complexity).

| Flow | Mode | Per student | 70 students Ã— 1 run | 70 students Ã— 2 runs/week |
|------|------|------------|---------------------|---------------------------|
| Flow B (batch) | Batch API (50% off) | ~$0.0007 | ~$0.05 | ~$0.10/week |
| Flow A (real-time) | Standard API | ~$0.0014 | ~$0.10 | ~$0.20/week |
| **Combined** | | | | **~$0.30/week** |

**Monthly: ~$1.20.** Essentially noise even at 5Ã— overestimate.

Notes:
- `.pptx` slides are intentionally sparse â€” likely *fewer* tokens than a `.docx` baseline
- Unit 4's qualitative criteria (e.g. "ErklÃ¤rungen verstÃ¤ndlich") need richer prompts than quantitative ones â€” partially offsets sparse text
- Prompt caching (rubric is repeated per student) would reduce Flow A costs further

---

## Rubric Format (Proposed)

Replace the current 1â€“4 scoring rubric with a checklist format for per-task feedback:

```json
{
  "graded_artifact": {
    "keyword": "computer-steckbrief",
    "format": [".docx", ".odt"],
    "criteria": [
      "Datei heiÃŸt genau 'Steckbrief-[Vorname].docx'",
      "Ãœberschrift 'Mein Computer-Steckbrief' vorhanden und als Ãœberschrift 1 formatiert",
      "Abschnitt 'Hardware' enthÃ¤lt mindestens 3 Komponenten mit je einer ErklÃ¤rung"
    ]
  }
}
```

LLM prompt instructs: for each criterion, return âœ“ or âœ— plus one short sentence of specific feedback. No overall score.

**Open:** define exact prompt structure and output format. Align with `grading-with-llm/conventions.md`.

---

## Growing Rubric Pattern (Authoring Convention)

Each task's `graded_artifact` criteria block = previous task's criteria + new criteria for this task.

Task 1 criteria: [file setup, heading]
Task 2 criteria: [file setup, heading, hardware section]
Task 3 criteria: [file setup, heading, hardware section, software section]
...
Capstone criteria: [everything]

Authoring rule: copy previous task's criteria block, append new criteria for current task. Never remove criteria from a previous task's block. This ensures the LLM always evaluates the full artifact state against what's been taught so far.

**Open:** should this be enforced/checked by `scripts/content_checker.py`?

---

## Implementation Work

1. **Per-step checkboxes**
   - JS: detect numbered `<li>` elements inside the `ðŸ“‹ Aufgabe:` section, inject checkboxes
   - DB: new table for step completion state (student_id, subtask_id, step_index, checked)
   - Minimal â€” no authoring changes, works on existing content immediately

2. **Upload UI per task**
   - File input widget on task page, shown after checkboxes (or always visible for tasks with `graded_artifact`)
   - Accepts formats listed in `graded_artifact.format`
   - Triggers LLM check on upload, displays checklist result inline

3. **LLM feedback call**
   - Extend `llm_grading.py` with new prompt style (checklist, not score)
   - Store result in new `artifact_feedback` table (student_id, subtask_id, timestamp, file_hash, feedback_json)
   - Re-upload â†’ new row, old feedback preserved (history)

4. **Rubric format change**
   - Extend `graded_artifact_json` schema: add `criteria` list alongside existing `rubric` (or replace â€” decide on backward compat)
   - Update import/export in `import_task.py`
   - Update `docs/task_json_format.md` and `docs/shared/lernmanager/conventions.md`

5. **Admin: capstone review + grade entry**
   - Admin view: list of capstone submissions per student, with LLM feedback visible as context
   - Grade input field (1â€“4 or school-specific scale â€” TBD)
   - Grade stored inâ€¦ new column on `student_subtask`? or separate `artifact_grade` table?

6. **Authoring**
   - Growing rubric convention documented in `docs/shared/mbi/conventions.md`
   - `content_checker.py` check: warn if task N criteria are a strict subset of task N-1 (i.e., nothing was added)

---

## Open Questions

**Architecture**
- [ ] DB table design for step checkbox state (student_id, subtask_id, step_index, checked)
- [ ] DB table / column design for artifact submissions â€” `artifact_feedback` table vs. column on `student_subtask`
- [ ] How does Flow B push results to Lernmanager? API endpoint? Direct DB write? File drop?
- [ ] Does the capstone upload replace or extend the existing quiz-based completion flow?
- [ ] Admin setting for opt-in: column on `klasse` table, or separate settings table?

**UX**
- [ ] Where does checklist feedback display â€” inline below upload widget, or separate result section?
- [ ] When does the upload prompt appear relative to the per-task quiz (if any)?
- [ ] What happens on re-upload â€” replace feedback or show history?
- [ ] What does the UI show when LLM feedback is disabled (opt-out state)? Upload still available, feedback just absent?

**Grading**
- [ ] LLM backend decision: OVHcloud Qwen3-32B (recommended â€” EU, â‚¬0.06/mo) vs. Mistral Large 3 (EU, â‚¬0.40/mo) vs. Mac Mini + WireGuard (cleanest DSGVO, school days only) vs. Anthropic (full DSGVO process)
- [ ] Benchmark Qwen3-32B (OVHcloud) on Unit 4 rubric for German-language checklist grading quality
- [ ] Obtain DPA from chosen EU provider (Mistral or OVHcloud) before enabling for any class
- [ ] Exact LLM prompt format for checklist output (align structure with grading-with-llm micro-prompts)
- [ ] Grade scale for teacher entry: 1â€“4 (grading-with-llm scale) or school's official 1â€“6?
- [ ] Path-aware grading for depth-model tasks (open in grading-with-llm too â€” see its conventions.md)

**Authoring / tooling**
- [ ] Should `content_checker.py` enforce growing rubric consistency (warn if task N adds no new criteria)?
- [ ] Backward compat: keep `rubric` field alongside new `criteria` list, or replace?

**DSGVO** (full analysis in Privacy section above)
- [ ] Obtain and review DPA (AVV) from Anthropic
- [ ] Verify Anthropic DPF certification or confirm SCCs cover EUâ†’US transfer
- [ ] Check ThÃ¼rDSB guidance on school cloud services
- [ ] Conduct or commission DPIA (Art. 35) â€” likely required
- [ ] Update school DatenschutzerklÃ¤rung (Art. 13 transparency obligation)
- [ ] DSB / Schulleitung sign-off before enabling for any class
- [ ] Data retention policy for `artifact_feedback` table â€” define and document
